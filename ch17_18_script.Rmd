---
title: "Chapters 17 and 18"
author: "David Kane"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(naniar)
library(mice)
library(broom)
library(tidyverse)

load("nes.rda")

x <- nes %>% 
  as_tibble() %>% 
  select(year, dvote, partyid7, real_ideo, race_adj, 
         age_discrete, educ1, female, income) %>% 
  mutate(gender = as.factor(ifelse(female == 1, "female", "male"))) %>% 
  mutate(race = as.factor(case_when(race_adj == 1 ~ "White",
                                    race_adj == 2 ~ "Black",
                                    TRUE ~ "Other"))) %>% 
  select(-female, -race_adj, -age_discrete, -educ1) %>% 
  rename(party = partyid7,
         ideology = real_ideo)


```



# **Missing Data Questions**

Let's spend time on missing data using the [**naniar**](https://naniar.njtierney.com/) and [**mice**]() packages. For **naniar**, Background reading [here](https://arxiv.org/pdf/1809.02264.pdf) about how to think about missing data in the context of the Tidyverse. See [here](https://uvastatlab.github.io/2019/05/01/getting-started-with-multiple-imputation-in-r/) and [here](https://thomasleeper.com/Rcourse/Tutorials/mi.html) for detailed examples of working with **mice**. Start with:

`install.packages("naniar")`

`install.packages("mice")`


# Scene 1

**Prompt:** Explore our data set `x` using the tools that we have already used. Don't use anything from **naniar** or **mice** yet. Answer these questions:

* Which variables have missing values?

* Is missingness large enough to be a worry?

```{r s1}
summary(x)

x %>% 
  sample_n(10)

x %>% 
  group_by(dvote) %>% 
  summarize(mn_income = mean(income))

```


# Scene 2

**Prompt:** Let's make some graphics. The **naniar** package comes with geom_* functions which can be used with **gpglot2** just like the built in geoms. It also has a variety of gg_* functions and miss_* functions. Let's start with `gg_miss_var()` and  `miss_var_summary()`. What do these show us?


```{r s2}
miss_var_summary(x)

gg_miss_var(x) + 
  labs(y = "Look at all the missing values")
```



# Scene 3

**Prompt:** `gg_miss_var()` also allows us to show things in terms of percentages. Explore that approach. As you can see, `dvote` is missing for 60% of our observations. Since the whole point of the exercise is to understand and explain the variation in `dvote`, this seems like a problem! 

```{r s3b}
gg_miss_var(x, show_pct = TRUE) +
  labs(y = "Viewing in terms of percentages is helpful")
```

Recall the discussion (p. 298ff) in RAOS about the different sorts of missingness. For each of the four types, write down its name and a scenario via which dvote might be missing which is consistent with that type of missingness. Give both words and some pseudo R-code in your explanation. Example:

* *missingness completely at random:* Imagine that the true dvote is replaced by NA randomly. Perhaps we had all our data on individual pieces of paper in a basket. Our puppy found the basket and ate 60% of the slips. The assignment mechanism by which specific dvotes were NA operated independent of anything else.

```{r s3a, eval=FALSE, echo=TRUE}
data_we_see <- true_data %>% 
  mutate(dvote = ifelse(runif() < 0.6, NA, dvote))
```

Do the same for the next three types of missing data. Describe each in terms of the "assignment mechanism" by which a given dvote became NA. Read [this](https://davidkane9.github.io/PPBDS/A-rubin-causal-model.html#the-assignment-mechanism) as a reminder.


# Scene 4

**Prompt:** use `gg_miss_var` to create a plot which is facetted by year. What explanations do you have for this pattern? Does this pattern make you more or less worried about all the missing data? That is, does this give any evidence as to what sort of missingness we are dealing with. And, deeper question, does this make you more or less worried about the inferences we are going to draw from our model?

```{r s4}
gg_miss_var(x,
            facet = year)
```



# Scene 5

**Prompt:** Check out the links about using **mice** to deal with missing data. Indeed, handling missing data in a more sophisticated fashion would make for an excellent extension for a final project. First, let's estimate this model:


```{r s5, echo=TRUE}
model_1 <- glm(data = x, dvote ~ ideology + gender, family = binomial)
model_1
```

Note how we only have about 6,660 observations, because of all the missing data. Use the tools in **mice** to multiply impute for the missing values and then estimate a new model with the collection of multiply imputed data sets. 

First, use the `mice()` function to create a set of multiple imputed data sets named `imp_1`. This is actually an object of class "mids" --- for (m)ultiply (i)mputed (d)ata (s)ets.

Second, explore `imp_1` by printing it. Then, use the `complete()` function to pull out the first full data set. Are there any missing values?

Third, use `complete()` to create a new data set, called `new` which stacks the 5 imputed data sets into a single data set, and turn that data set into a tibble, for ease of exploration.

Fourth, run the same regression as we used to create `model_1` for each of the five imputed data sets. How different are the coeficients of gendermale and ideology across the five? What does that tell us? (Hint: There are many ways to do this, but I went with something [along these lines](https://davidkane9.github.io/PPBDS/13-classification.html#fitting-many-models-using-map-1).)


# **Causal Inference Questions**

# Scene 1

**Prompt:** Suppose you are interested in the effect of vending machines on childhood obesity. What controlled experiment would you want to do (in a world without ethical, logistical, or financial constraints) to evaluate this question?


# Scene 2

**Prompt:** Suppose you are interested in the effect of smoking on lung cancer. What controlled experiment could you plausibly perform (in the real world) to evaluate this effect?


# **Final Project**

We want the final projects for the class to be as good as they would have been.

# Scene 1

**Prompt:** Each person in your group should share their screen and give a tour of their final project repo. We will devote 10 minutes to this exercise, so each of N the persons should share their screen for 10/N minutes.



